# Training Configuration for LoRA Fine-tuning
model:
  pretrained_model: "stabilityai/stable-diffusion-xl-base-1.0"
  vae_model: "madebyollin/sdxl-vae-fp16-fix"
  revision: null
  variant: "fp16"

lora:
  rank: 64
  alpha: 64
  target_modules:
    - "to_q"
    - "to_k"
    - "to_v"
    - "to_out.0"
  dropout: 0.0

training:
  output_dir: "./output/lora"
  seed: 42
  resolution: 1024
  train_batch_size: 1
  gradient_accumulation_steps: 4
  learning_rate: 1.0e-4
  lr_scheduler: "cosine"
  lr_warmup_steps: 100
  max_train_steps: 1000
  mixed_precision: "fp16"
  gradient_checkpointing: true
  use_8bit_adam: true
  max_grad_norm: 1.0
  save_steps: 250
  validation_steps: 100

dataset:
  train_data_dir: "./data/processed"
  caption_column: "text"
  image_column: "image"
  center_crop: true
  random_flip: true

logging:
  use_wandb: false
  project_name: "geovera-image-generator"
  log_every: 10
